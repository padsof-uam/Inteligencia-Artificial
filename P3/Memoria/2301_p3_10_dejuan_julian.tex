\documentclass[nochap]{apuntes}

\usepackage{alltt}
\author{Guillermo Julián y Víctor de Juan}
\date{10-Abril}
\title{Inteligencia Artificial - Practica 3}

\begin{document}
\maketitle

\section*{Parte B}
\subsection*{Pregunta B1. Función de Evaluación}
\subsubsection*{Apartado B1.1}

Todas nuestras funciones de evaluación siguen el mismo formato. Varios factores dependientes del tablero que podían ser influyentes, ponderados según su importancia.

\subsubsection*{Fundamentos, razonamiento y pruebas}
Tras jugar varias partidas descubrimos factores importantes

\begin{itemize}
\item El número de hoyos ocupados.
\item El máximo de semillas en un hoyo.
\item Cuántos hoyos hay libres.
\item El máximo de semillas que puedo robar encadenadamente. (Contando con que al robar hay que seguir sembrando y se pueden volver a robar)
\item El número de hoyos en los que no nos pueden robar semillas (por tener más de 4 semillas o por no tener ninguna)
\item El número de hoyos en los que si se pueden robar semillas.
\item El número de hoyos con 1 semilla (si tenemos todos los hoyos con una única semilla es muy fácil perder, porque en cuanto nos roben una, siembran y nos roban la siguiente (si el oponente tiene semillas en sus hoyos claro)
\end{itemize}

Estos factores son computables para ambos jugadores de la partida. La implementación de estas heurísitcas se encuentra en el \textit{*heuristics*}

El problema que nos encontramos fue cómo ponderar los diversos factores. Lo que hemos hecho para resolver este sistema ha sido utilizar un algoritmo de optimización llamado \textit{Simulated-annealing}. El empleo de este algoritmo nos posibilitó encontrar la mejor combinación de ponderaciones para los factores.

\paragraph{Simulated Annealing}
La implementación de este algoritmo se encuentra repartida entre los ficheros \textit{siman.cl} (que contiene el algoritmo general de simmulated annealing) y \textit{simancala.cl} (que es la concreción del algoritmo para partidas de mancala).

El resultado de la ejecución de \textit{simancala.cl} es una lista de números entre -1 y 1. Por ejemplo -0.83 significa partida ganada en 17 turnos, y de manera similar, 0.36 significa partida perdida en 0.64 turnos. De esta forma cuanto más negativa sea la media de las partidas jugadas mejor. 


\paragraph{Pruebas}

Implementamos pequeñas modificaciones en el código recibido para poder tener una función (partida-SA-all-games) que dada una lista de ponderaciones jugara contra los jugadores de referencia \textit{Bueno} y \textit{Regular} a profundidades 1 y 2 (por no emplear demasiado tiempo en la simulación). 

Para ello fue necesario crear una función de evaluación de cada jugador que también recibiera una lista de ponderaciones aunque luego no la usara y una función de minimax que recibiera como argumento las ponderaciones además del jugador de pruebas que tuviera una fución de evaluación dependiente de las heurísticas definidas en \textit{heuristics.cl}. Todas estas funciones están agrupadas en el fichero Practica3-SA.cl. 


\subsubsection*{Apartado B1.2}
\subsubsection*{Pruebas de tiempo}
Para medir los tiempos de ejecución de las heurísticas hicimos pruebas del estilo:
\begin{alltt}
(setq mi-posicion (list '(1 0 1 3 3 4 0 3) (reverse '(4 0 3 5 1 1 0 1))))
(setq estado (crea-estado-inicial 0 mi-posicion))
(time (minimax estado 4 'mi-f-eval))
(time (minimax estado 4 'f-eval-Bueno))
\end{alltt}

Y comparando las salidas.

\begin{alltt}
Real time: 0.991875 sec.
Run time: 0.99086 sec.
Space: 11696480 Bytes
GC: 13, GC time: 0.053439 sec.

Real time: 1.322388 sec.
Run time: 1.321234 sec.
Space: 15407168 Bytes
GC: 18, GC time: 0.07669 sec.
\end{alltt}

Observamos que \textit{mi-f-eval} es más rápida que la función de evaluación del bueno, pero no llega al 50\% recomendado.

\subsection*{Pregunta B2. Minimax y Minimax a-b}
\subsubsection*{Apartado B2.1}

\paragraph{Explicación el código entregado:}

Está comentado el código de la función minimax.

\subsubsection*{Apartado B2.2}
\paragraph{Implementa la poda alfa-beta:}
Está implementado en el código.

\subsubsection*{Apartado B2.3}

\paragraph{Compare el tiempo que tarda un jugador utilizando minimax y utilizando minimax con poda alfa-beta. Comente los resultados.\\\\}


La poda tarda mucho menos en el caso de profundidad 2 todas las veces que lo comprobamos. A continuación se muestran los resultados de una ejecución típica:
\begin{alltt}
(time (minimax estado 2 'f-eval-Regular)) 
  ; Run time: 0.016814 sec.
(time (minimax-a-b estado 2 'f-eval-Regular)) 
  ; Run time: 0.013248 sec.
\end{alltt}

Si aumentamos la profundidad, cada vez se nota más la diferencia. Inlustramos con 2 ejemplos de ejecuciones:

\begin{alltt}
(time (minimax estado 5 'f-eval-Regular)) 
  ; Run time: 1.279404 sec.
  ; Run time: 1.293841 sec.

(time (minimax-a-b estado 5 'f-eval-Regular)) 
  ; Run time: 0.644046 sec.
  ; Run time: 0.638829 sec.

\end{alltt}

Se observa una variación de los tiempos a pesar de ser la misma función evaluada. Esto se debe a que al medir tiempos de ejecución, el programa puede tardar más o menos dependiendo del estado de la máquina.

\subsubsection*{Apartado B2.4}
\paragraph{ Modifique el orden el que se exploran las jugadas. Comente el efecto que tiene en la poda alfa-beta modificar dicho orden.\\\\}

Con el mismo tablero que en las pruebas anteriores, utilizando la función \textit{rever-minimax-a-b} (que simplemente hace un \textit{reverse} de la lista de los sucesores) obtenemos unos resultados ligeramente mejores (a pesar de haber tenido que emplear la función reverse que no será un tiempo muy despreciable).

Tanto a profundidad 2 como a profundidad 5 se nota la diferencia (aunque a mayor profuncidad mayor diferencia):

\begin{alltt}
(time (minimax-a-b estado 2 'f-eval-Regular)) 
  ; Run time: 0.015907 sec.
  ; Run time: 0.015142 sec.
  ; Run time: 0.015485 sec.

(time (rever-minimax-a-b estado 2 'f-eval-Regular)) 
  ; Run time: 0.010086 sec.
  ; Run time: 0.007218 sec.
  ; Run time: 0.015323 sec.

(time (minimax-a-b estado 5 'f-eval-Regular)) 
  ; Run time: 0.666123 sec.
  ; Run time: 0.646479 sec.

(time (rever-minimax-a-b estado 5 'f-eval-Regular)) 
  ; Run time: 0.293419 sec.
  ; Run time: 0.288963 sec.
\end{alltt}

Lo que está pasando es que al hacer \textit{reverse} de la lista estamos explorando antes la mejor rama y podando el resto, evitando tiempo de procesado inútil.

\subsubsection*{Apartado B2.5}
Si se pudiera ordenar los nodos a explorar de mejor a peor según la heurística, se podarían siempre todas las opciones menos la primera, teniendo simplemente un algoritmo de recorrido y no de búsqueda. El problema es que no podemos saber de antemano que estado es mejor que otro.

Se podría definir una posible regla que fuera ordenar aleatoriamente la lista de sucesores u ordenar según la heurística pero no se pueden evaluar los nodos antes de generarlos, asique sólo cabría esperar que el aleatorio fuera suficiente o una vez generados los sucesores evaluarlos con una heurística rápida e intentar ordenarlos así, una vez generados.

\end{document}